name: E2E tests

on:
  push:
    branches:
      - main
  pull_request:
  schedule:
    - cron: "0 8 * * *"  # Run daily at 12AM PST (adjusted for UTC)

jobs:
  tp-run:
    name: Submit workloads
    runs-on: ubuntu-22.04
    env:
      ARTIFACT_DIR: gs://torchprime-e2e-tests/${{ github.job }}/${{ github.run_id }}-${{ github.run_attempt }}
    outputs:
      llama-3-8b-name: ${{ steps.run-llama-3-8b.outputs.name }}
      llama-3_1-8b-sa-name: ${{ steps.run-llama-3_1-8b-SplashAttention.outputs.name }}
      llama-3_1-8b-scan-offload-name: ${{ steps.run-llama-3_1-8b-scan-offload.outputs.name }}
      llama-3-8b-2d-name: ${{ steps.run-llama-3-8b-2d.outputs.name }}
      llama-3-8b-2-slice-name: ${{ steps.run-llama-3-8b-2-slice.outputs.name }}
      mixtral-8x7b-name: ${{ steps.run-mixtral-8x7b.outputs.name }}
      artifact-dir: ${{ steps.artifacts.outputs.artifact_dir }}
    steps:
      - name: Record artifact dir
        id: artifacts
        run: |
          echo "Artifact dir: $ARTIFACT_DIR"
          echo "artifact_dir=$ARTIFACT_DIR" >> "$GITHUB_OUTPUT"
      - name: Maximize build space
        uses: AdityaGarg8/remove-unwanted-software@v4.1
        with:
          remove-dotnet: 'true'
          remove-android: 'true'
          remove-haskell: 'true'
          remove-codeql: 'true'
      - uses: actions/checkout@v4
      - uses: ./.github/actions/e2e-setup
        with:
          gcp_project: ${{ vars.GCP_PROJECT }}
          gcp_zone: ${{ vars.GCP_ZONE }}
          xpk_cluster_name: ${{ vars.XPK_CLUSTER_NAME }}
          tpu_type: ${{ vars.TPU_TYPE }}
          artifact_dir: ${{ env.ARTIFACT_DIR }}
          gcp_sa_key: ${{ secrets.GCP_SA_KEY }}

      - name: Run Llama 3.0 8B
        id: run-llama-3-8b
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3-8b)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3-8b \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.1 8B (Splash Attention)
        id: run-llama-3_1-8b-SplashAttention
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3dot1-8b-sa)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3.1-8b \
            model.attention_kernel=splash_attention \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.1 8B (Scan + Offload)
        id: run-llama-3_1-8b-scan-offload
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3dot1-8b-sa)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3.1-8b \
            model/remat=llama-scan-offload \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.0 8B (2D sharding)
        id: run-llama-3-8b-2d
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3-8b-2d)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=llama-3-8b \
            model/sharding=llama-fsdp-tp \
            global_batch_size=8 \
            ici_mesh.fsdp=2 \
            ici_mesh.tensor=2 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Mixtral 8x7B
        id: run-mixtral-8x7b
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py mixtral-8x7b)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run \
            --name $name \
            torchprime/torch_xla_models/train.py \
            model=mixtral-8x7b \
            model.num_hidden_layers=16 \
            global_batch_size=8 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

      - name: Run Llama 3.0 8B (2 slice)
        id: run-llama-3-8b-2-slice
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          XLA_IR_DEBUG: 1
          XLA_HLO_DEBUG: 1
        run: |
          name=$(e2e_testing/gen_name.py llama-3-8b-2-slice)
          echo "name=$name" >> "$GITHUB_OUTPUT"
          tp run \
            --name $name \
            --num-slices 2 \
            torchprime/torch_xla_models/train.py \
            model=llama-3-8b \
            model/sharding=llama-fsdp \
            global_batch_size=16 \
            dcn_mesh.fsdp=2 \
            ici_mesh.fsdp=4 \
            dataset_config_name=wikitext-2-raw-v1 \
            profile_step=3 \
            max_steps=15

  # Validate the results of the workloads
  #
  # Each workload has a step time lower bound and upper bound.
  #
  # The initial bounds and confidence interval comes from a one-time analysis in Apr 2025:
  # https://docs.google.com/spreadsheets/d/1VS_0tkmmUwT22F2eeqqIOuExiBwIIbqn58MXw3RMe7M/edit?usp=sharing
  #
  # Note that we use the convention of
  # `step_time_upper_bound - step_time_lower_bound = confidence_interval * 2`.

  llama-3-8b:
    name: Llama 3.0 8B
    needs: tp-run
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: ${{ needs.tp-run.outputs.llama-3-8b-name }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      # Confidence interval: 0.04162
      step_time_lower_bound: 2.733225455
      step_time_upper_bound: 2.816465455
    secrets: inherit

  llama-3_1-8b-sa:
    name: Llama 3.1 8B (Splash Attention)
    needs: tp-run
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: ${{ needs.tp-run.outputs.llama-3_1-8b-sa-name }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      # Confidence interval: 0.10575
      step_time_lower_bound: 2.345504545
      step_time_upper_bound: 2.557004545
    secrets: inherit

  llama-3_1-8b-scan-offload:
    name: Llama 3.1 8B (Scan + Offload)
    needs: tp-run
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: ${{ needs.tp-run.outputs.llama-3_1-8b-scan-offload-name }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      # Confidence interval: 0.03137
      step_time_lower_bound: 2.789257273
      step_time_upper_bound: 2.851997273
    secrets: inherit

  llama-3-8b-2d:
    name: Llama 3.0 8B (2D sharding)
    needs: tp-run
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: ${{ needs.tp-run.outputs.llama-3-8b-2d-name }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      # Confidence interval: 0.03373
      step_time_lower_bound: 3.33917
      step_time_upper_bound: 3.40663
    secrets: inherit

  llama-3-8b-2-slice:
    name: Llama 3.0 8B (2 slice)
    needs: tp-run
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: ${{ needs.tp-run.outputs.llama-3-8b-2-slice-name }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      # Confidence interval: 15.65875
      step_time_lower_bound: 4.252995455
      step_time_upper_bound: 35.57049545
    secrets: inherit

  mixtral-8x7b:
    name: Mixtral 8x7B
    needs: tp-run
    uses: ./.github/workflows/reusable_e2e_check.yml
    with:
      jobset_name: ${{ needs.tp-run.outputs.mixtral-8x7b-name }}
      artifact_dir: ${{ needs.tp-run.outputs.artifact-dir }}
      # Confidence interval: 0.03167
      step_time_lower_bound: 3.13563
      step_time_upper_bound: 3.19897
    secrets: inherit
